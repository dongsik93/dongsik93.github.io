---
layout: post
title: "AI"
subtitle: "AI 여러가지 개념"
date: 2019-06-21 18:00:00 +0900
categories: til
tags: etc
comments: true
---


## AI Learning



### Logistic Regression(로지스틱 회귀분석)



- Logit function 을 사용한 회귀분석
- 기존의 회귀식
  - 좌변과 우변에서 계산된 값이 모두 연속형인 수를 가정
  - 좌변과 우변에서 계산된 값이 연속형
- 로지스틱회귀
  - 우변은 연속형인 값이 나오지만 좌변은 이산형인 값들만 나옴
  - 우변의 계산값이 좌변의 계산값과 안맞아 등호가 성립할 수 없게되는 문제가 생김
  - 따라서 좌변의 y값을 어떠한 처리를 통해 범위를 맞춰주는 작업을 해줘야 하는데 이것이 Logit(Log + Odds)변환이다.



- Logit 변환이란
  - 오즈에 로그를 씌운것
  - 일반회귀식으로 0 또는 1(이산형)을 바로 예측할 수는 ㅇ벗으므로, y값(1,0)의 확률을 이용해 오즈비로 변환하여, 로그를 취한다면 불가능했던 예측이 가능
- Odds란
  - 어떠한 일이 일어날 확률(1이될 확률)을 일어나지 않을 확률(1-p)로 나눈것을 말함
  - 각 독립변수에 대해 실패/성곡에 대한 확률을 구한 뒤, 각각 구함

- 로지스틱 회귀 분석은 데이터마이닝, 기계학습등 다양한 분야에서 분류 및 예측을 위한 모델로서 폭 넓게 사용되고 있다.



### Attention Is All You Need



- ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762){: class="underlineFill"}
- 구글 브레인, 구글 리서치 팀에서 발표한 신경망을 이용한 기계번역(NMT: Neural Machine Translation)에 대한 논문이다.
- Recurrent, Convolution을 사용하지 않고 Attention"만"을 사용하는 간단한 신경망 구조를 통해 기계 번역 분야(특히, 영어에서 독일어로 번역)에서 state-of-the-art- 성능을 얻음과 동시에 computation cost를 줄일 수 있었다고 한다.

- 이 논문에서 정의하는 **attention**이란 query와 key-value pair들을 받아서 output에 맵핑해 주는 함수이다.

![img](http://postfiles10.naver.net/MjAxNzA2MjNfMTEw/MDAxNDk4MTkzNzU4ODg1.6Eyf6ml_OT-4I7_KyWwBe5ueXFEdyEai71hhUiAwWeIg.bJjdwZJPwCy0xEhZBet_MK2Gl7wHkC9Fdco9oIrgQSgg.PNG.hist0134/image.png?type=w773)

- 위 공식이 attention을 수식으로 나타낸 것인데, __Q에 맞게 K를 이용해 V에 가중치를 주는 기법이다.__
- previous layer의 output(Q)에 맞게 previous layer의 output(K)를 이용해 current layer의 output(V)에 가중치를 구하는 것이다.
- attention mechanism이 중요한 이유
  - 각각 layer마다 필요로 하는 총 computin cost가 줄어든다
  - 병렬화가 가능한 computation이 늘어난다
  - 신경망 내에서 long-range dependencies를 잇는 path length가 줄어든다
  - 해석가능한 모델을 만들기 위함

- 평가
  - Recurrent Cell과 Convolution을 벗어나려는 좋은 시도, 



- 어렵다....무슨말인지 1도 모르겠다......
- AI는 어렵다...


- 참고사이트 : [hist0613님 블로그](http://blog.naver.com/PostList.nhn?blogId=hist0134){: class="underlineFill"}


